{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "demo_computing_gradients.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM33B2WPHftppsuNVwYN1CR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/siddharthchd/introml/blob/master/unit7/demo_computing_gradients.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bzelhm-oMf0h"
      },
      "source": [
        "### Computing Gradients"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eRna_1opMmEk"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kCb8vEuhMm49"
      },
      "source": [
        "#### Example 1 : A simple vector-input function\n",
        "\n",
        "Suppose f(w) = w_0^2 + 2w_0w_1^3. Then the function below computes f(w) and its gradients"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xiHAME5xM0Ls",
        "outputId": "6ea8baef-9c64-4b5e-bb2b-8941be563969",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "def feval(w):\n",
        "\n",
        "    # function\n",
        "    f = w[0] ** 2 + 2 * w[0] * (w[1] ** 3)\n",
        "\n",
        "    # gradient\n",
        "    df0 = 2 * w[0] + 2 * (w[1] ** 3)\n",
        "    df1 = 6 * w[0] * (w[1] ** 2)\n",
        "    fgrad = np.array([df0, df1])\n",
        "\n",
        "    return f, fgrad\n",
        "\n",
        "# point to evaluate\n",
        "w = np.array([2, 4])\n",
        "f, fgrad = feval(w)\n",
        "\n",
        "print('f        = %f' % f)\n",
        "print('fgrad    = {}'.format(fgrad))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "f        = 260.000000\n",
            "fgrad    = [132 192]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tU0KFa-ZNdWP"
      },
      "source": [
        "#### Example 2 : Non-Linear least squares for an exponential model\n",
        "\n",
        "Consider an exponential model \\\\\n",
        "\n",
        "\\begin{align*}\n",
        "    yhat = a * exp(-b * x) \\\\\n",
        "\\end{align*}\n",
        "\n",
        "for parameters $w = [a, b]$. Given training data $(x[i], y[i])$ a natural loss function is given by : \\\\\n",
        "\n",
        "\\begin{align*}\n",
        "    J(w) := \\sum_i (y[i] - yhat[i])^2, \\\\ yhat[i] = a * exp(-b * x[i]) \\\\\n",
        "\\end{align*}\n",
        "\n",
        "\n",
        "The following code computes the loss function $J(w)$ and its gradient $\\frac{dJ}{dw}$\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SkMTkvVXOboT",
        "outputId": "151a629b-9099-489c-d4b7-d1a73472f6d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "def Jeval(w):\n",
        "\n",
        "    # unpack vector\n",
        "    a = w[0]\n",
        "    b = w[1]\n",
        "\n",
        "    # compute the loss function\n",
        "    yerr = y - a * np.exp(-b * x)\n",
        "    J = 0.5 * np.sum(yerr ** 2)\n",
        "\n",
        "    # compute the gradient\n",
        "    dJ_da = -np.sum(yerr * np.exp(-b * x))\n",
        "    dJ_db = np.sum(yerr * a * x * np.exp(-b * x))\n",
        "    Jgrad = np.array([dJ_da, dJ_db])\n",
        "\n",
        "    return J, Jgrad\n",
        "\n",
        "# generate some random data\n",
        "ny = 100\n",
        "y = np.random.randn(ny)\n",
        "x = np.random.rand(ny)\n",
        "\n",
        "# some arbitrary parameters to compute the gradient at\n",
        "a = 1\n",
        "b = 2\n",
        "w = np.array([a, b])\n",
        "\n",
        "J, Jgrad = Jeval(w)\n",
        "print('Jgrad = {}'.format(Jgrad))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Jgrad = [21.85747909 -4.74780678]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9dJ8TwtjPeXg"
      },
      "source": [
        "#### Example 3 : Loss function for a Log-Linear Model\n",
        "\n",
        "Now suppose we have a lograithmic linear model : \\\\\n",
        "\\begin{align*}\n",
        "\\hat{y} & = log(z_i) \\\\\n",
        "z & = w_0 + \\sum_j w_j x_j \\\\\n",
        "\\end{align*}\n",
        "\n",
        "\n",
        "If we have data X, y, the prediction on sample i is : \\\\\n",
        "\n",
        "\\begin{align*}\n",
        "\\hat{y_i} & = log(z_i) \\\\\n",
        "z_i & = w_0 + \\sum_j w_j * x_{ij} \\\\\n",
        "\\end{align*}\n",
        "\n",
        "Suppose we use MSE loss:\n",
        "\n",
        "\\begin{align*}\n",
        "J(w) = \\sum_i (y_i - yhat_i )^2 \\\\\n",
        "\\end{align*}\n",
        "\n",
        "To compute the components $\\frac{dJ}{dw_j}$, first write $z = Aw$ where $A=[1 X]$, the matrix with ones on the first column. Then, $\\frac{dz_i}{dw_j} = A_{ij}$. Also, $\\frac{d\\hat{y_i}}{dz_i} = \\frac{1}{z_i}$, so with the multi-variable chain rule:\n",
        "\n",
        "\\begin{align*}\n",
        "\\frac{dJ}{dw_j} & = \\sum_i \\frac{dJ}{d \\hat{y_i}} * \\frac{d \\hat{y_i}}{dw_j} \\\\\n",
        "& = \\sum_i (\\frac{dJ}{d \\hat{y_i}}) * (\\frac{d \\hat{y_i}}{dz_i}) * (\\frac{dz_i}{dw_j})\\\\\n",
        "& = 2 * \\sum_i (\\hat{y_i} - y_i) * \\frac{1}{z_i} * A_{ij} \\\\\n",
        "\\end{align*}\n",
        "\n",
        "We can implement the loss and gradient computation as follows:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kFkfibIoOdGo"
      },
      "source": [
        "def  Jeval(w, X, y):\n",
        "\n",
        "    # Create matrix A = [1, X]\n",
        "    n = X.shape[0]\n",
        "    A = np.column_stack((np.ones(n), X))\n",
        "\n",
        "    # Compute function\n",
        "    z = A.dot(w)\n",
        "    yhat = np.log(z)\n",
        "    J = np.sum((y - yhat) ** 2)\n",
        "\n",
        "    # Compute gradient\n",
        "    dj_dz = 2 * (yhat - y) / z\n",
        "    Jgrad = A.T.dot(dj_dz)\n",
        "\n",
        "    return J, Jgrad"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cin79rvaTNM5"
      },
      "source": [
        "Whenever you implement a gradient you should always check the gradients. Errors in the gradient are the number \n",
        "\n",
        "- Take two points w0 and w1 close to one another\n",
        "- Verify that J(w1) - J(w0) is close to Jgrad(w0).dot(w1 - w0)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qfaSWTHiTokh",
        "outputId": "0ee16c9e-8b3b-4bc1-9b5c-33a439b220f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Generate random positive data\n",
        "n = 100\n",
        "d = 5\n",
        "X = np.random.uniform(0, 1, (n, d))\n",
        "w0 = np.random.uniform(0, 1, (d+1, ))\n",
        "y = np.random.uniform(0, 2, (n, ))\n",
        "\n",
        "# Compute function and gradient at point w0\n",
        "J0, Jgrad0 = Jeval(w0, X, y)\n",
        "\n",
        "# Take a small pertubation\n",
        "step = 1e-4\n",
        "w1 = w0 + step * np.random.normal(0, 1, (d+1,))\n",
        "\n",
        "# Evaluate the function at perterbed point\n",
        "J1, Jgrad1 = Jeval(w1, X, y)\n",
        "\n",
        "dJ= J1 - J0\n",
        "dJ_est = Jgrad0.dot(w1 - w0)\n",
        "print('Actual differnece :  %12.4e' % dJ)\n",
        "print('Estimated differnece :  %12.4e' % dJ_est)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Actual differnece :   -1.7047e-02\n",
            "Estimated differnece :   -1.7050e-02\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DAoNJyFvUqUJ"
      },
      "source": [
        "#### Example 3 : A function of a matrix\n",
        "\n",
        "Suppose $f(W) = a'*W*b$. Then, $ \\partial f(W) = a*b.T$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q1FzvbaQVG7a"
      },
      "source": [
        "def feval(W, a, b):\n",
        "\n",
        "    # Function\n",
        "    f = a.dot(W.dot(b))\n",
        "\n",
        "    # Gradient -- using python broadcasting\n",
        "    fgrad = a[:, None] * b[None, :]\n",
        "\n",
        "    return f, fgrad\n",
        "\n",
        "# Some random data\n",
        "m = 4\n",
        "n = 3\n",
        "W = np.random.randn(m, n)\n",
        "a = np.random.randn(m)\n",
        "b = np.random.randn(n)\n",
        "\n",
        "f, fgrad = feval(W, a, b)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4b5Jyhw7V6cN"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}